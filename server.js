// === API FULLTECH - Memoria Sem√°ntica Inteligente ===
// Desarrollado por Junior L√≥pez - FULLTECH SRL

import express from "express";
import cors from "cors";
import fetch from "node-fetch";
import pkg from "pg";
const { Pool } = pkg;

const app = express();
app.use(cors());
app.use(express.json());

// =========================================================
// üíæ Conexi√≥n PostgreSQL (nombre real de la base)
// =========================================================
const pool = new Pool({
  host: "postgresql_postgres-vector", // nombre del servicio en EasyPanel
  port: 5432,
  database: "vector_memory", // nombre exacto de la BD
  user: "n8n_user",
  password: "Ayleen10.yahaira",
  ssl: false,
});

// =========================================================
// üîë Clave de OpenAI (desde variable de entorno)
// =========================================================
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
if (!OPENAI_API_KEY) {
  console.warn("‚ö†Ô∏è Advertencia: Falta la variable OPENAI_API_KEY en el entorno.");
}

// =========================================================
// üß© Verificaci√≥n y creaci√≥n de tablas vectoriales
// =========================================================
async function ensureTables() {
  const client = await pool.connect();
  try {
    await client.query(`
      CREATE EXTENSION IF NOT EXISTS "pgcrypto";
      CREATE EXTENSION IF NOT EXISTS "vector";

      CREATE TABLE IF NOT EXISTS fulltech_conversations (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        title TEXT DEFAULT 'Conversaci√≥n sin t√≠tulo',
        created_at TIMESTAMPTZ DEFAULT NOW()
      );

      CREATE TABLE IF NOT EXISTS fulltech_messages (
        id BIGSERIAL PRIMARY KEY,
        conversation_id UUID REFERENCES fulltech_conversations(id) ON DELETE CASCADE,
        role TEXT NOT NULL,
        content TEXT NOT NULL,
        embedding VECTOR(1536),
        created_at TIMESTAMPTZ DEFAULT NOW()
      );

      CREATE INDEX IF NOT EXISTS idx_embedding_vector
      ON fulltech_messages
      USING ivfflat (embedding vector_l2_ops)
      WITH (lists = 100);
    `);
    console.log("‚úÖ Tablas verificadas y listas (con soporte vectorial).");
  } catch (err) {
    console.error("‚ùå Error al crear/verificar tablas:", err.message);
  } finally {
    client.release();
  }
}

// =========================================================
// üß† Generar Embeddings con OpenAI
// =========================================================
async function generarEmbedding(texto) {
  try {
    if (!OPENAI_API_KEY) return [];

    const response = await fetch("https://api.openai.com/v1/embeddings", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        input: texto,
        model: "text-embedding-3-small",
      }),
    });

    const data = await response.json();
    if (!data?.data?.[0]?.embedding) {
      console.error("‚ö†Ô∏è Respuesta inesperada de OpenAI:", data);
      return [];
    }

    return data.data[0].embedding;
  } catch (error) {
    console.error("‚ùå Error generando embedding:", error.message);
    return [];
  }
}

// =========================================================
// üü¢ ENDPOINTS B√ÅSICOS
// =========================================================

// Ruta de prueba
app.get("/ping", (req, res) => {
  res.json({ status: "‚úÖ Servidor activo y corriendo correctamente" });
});

// Crear nueva conversaci√≥n
app.post("/conversations", async (req, res) => {
  const { title } = req.body;
  try {
    const result = await pool.query(
      "INSERT INTO fulltech_conversations (title) VALUES ($1) RETURNING *",
      [title || "Nueva conversaci√≥n"]
    );
    res.json(result.rows[0]);
  } catch (err) {
    console.error("‚ö†Ô∏è Error creando conversaci√≥n:", err.message);
    res.status(500).json({ error: "Error al crear conversaci√≥n" });
  }
});

// =========================================================
// üí¨ Guardar mensaje con embedding vectorial
// =========================================================
app.post("/messages", async (req, res) => {
  const { conversation_id, role, content } = req.body;
  try {
    if (!conversation_id || !content) {
      return res.status(400).json({ error: "Faltan datos requeridos" });
    }

    const embedding = await generarEmbedding(content);
    const vector = Array.isArray(embedding) && embedding.length
      ? `[${embedding.join(",")}]`
      : null;

    const query = vector
      ? `INSERT INTO fulltech_messages (conversation_id, role, content, embedding)
         VALUES ($1, $2, $3, $4::vector)`
      : `INSERT INTO fulltech_messages (conversation_id, role, content)
         VALUES ($1, $2, $3)`;

    const params = vector
      ? [conversation_id, role || "user", content, vector]
      : [conversation_id, role || "user", content];

    await pool.query(query, params);

    console.log(`üíæ Mensaje guardado (${role || "user"}): ${content}`);
    res.json({ success: true });
  } catch (err) {
    console.error("‚ö†Ô∏è Error guardando mensaje:", err.message);
    res.status(500).json({ error: "Error al guardar mensaje" });
  }
});

// =========================================================
// üìú Obtener historial de conversaci√≥n
// =========================================================
app.get("/messages/:conversation_id", async (req, res) => {
  const { conversation_id } = req.params;
  try {
    const result = await pool.query(
      "SELECT * FROM fulltech_messages WHERE conversation_id = $1 ORDER BY created_at ASC",
      [conversation_id]
    );
    res.json(result.rows);
  } catch (err) {
    console.error("‚ö†Ô∏è Error obteniendo mensajes:", err.message);
    res.status(500).json({ error: "Error al obtener mensajes" });
  }
});

// =========================================================
// üîç B√∫squeda sem√°ntica (recuperar contexto relevante)
// =========================================================
app.post("/memory/search", async (req, res) => {
  const { conversation_id, embedding, limit = 5 } = req.body;
  try {
    const result = await pool.query(
      `
        SELECT role, content, created_at
        FROM fulltech_messages
        WHERE conversation_id = $1
        AND embedding IS NOT NULL
        ORDER BY embedding <-> $2
        LIMIT $3
      `,
      [conversation_id, embedding, limit]
    );
    res.json(result.rows);
  } catch (err) {
    console.error("‚ö†Ô∏è Error en b√∫squeda sem√°ntica:", err.message);
    res.status(500).json({ error: "Error al buscar memoria" });
  }
});

// =========================================================
// ü§ñ NUEVO ENDPOINT /chat (IA con memoria vectorial)
// =========================================================
app.post("/chat", async (req, res) => {
  try {
    const { conversation_id, user_message } = req.body;

    if (!conversation_id || !user_message) {
      return res.status(400).json({ error: "Faltan datos: conversation_id o user_message" });
    }

    // 1Ô∏è‚É£ Guardar mensaje del usuario
    const userEmbedding = await generarEmbedding(user_message);
    const vectorUser = `[${userEmbedding.join(",")}]`;

    await pool.query(
      "INSERT INTO fulltech_messages (conversation_id, role, content, embedding) VALUES ($1, 'user', $2, $3::vector)",
      [conversation_id, user_message, vectorUser]
    );

    // 2Ô∏è‚É£ Buscar los 5 mensajes m√°s parecidos (contexto)
    const { rows: contextRows } = await pool.query(
      `
      SELECT content
      FROM fulltech_messages
      WHERE conversation_id = $1
      AND embedding IS NOT NULL
      ORDER BY embedding <-> $2::vector
      LIMIT 5
      `,
      [conversation_id, vectorUser]
    );

    const context = contextRows.map(r => r.content).join("\n");

    // 3Ô∏è‚É£ Crear prompt con contexto
    const fullPrompt = `
Eres Fulltech AI, asistente t√©cnico y vendedor de Fulltech SRL.
Responde de forma clara, profesional y con tono dominicano si aplica.

Contexto previo:
${context}

Cliente dice:
${user_message}
`;

    // 4Ô∏è‚É£ Llamar a OpenAI
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages: [
          { role: "system", content: "Eres Fulltech AI, un asistente t√©cnico profesional de Fulltech SRL." },
          { role: "user", content: fullPrompt },
        ],
      }),
    });

    const data = await response.json();
    const assistant_message =
      data?.choices?.[0]?.message?.content || "Lo siento, no pude generar una respuesta.";

    // 5Ô∏è‚É£ Guardar respuesta de la IA
    const aiEmbedding = await generarEmbedding(assistant_message);
    const vectorAI = `[${aiEmbedding.join(",")}]`;

    await pool.query(
      "INSERT INTO fulltech_messages (conversation_id, role, content, embedding) VALUES ($1, 'assistant', $2, $3::vector)",
      [conversation_id, assistant_message, vectorAI]
    );

    // 6Ô∏è‚É£ Devolver respuesta al cliente
    res.json({
      success: true,
      reply: assistant_message,
      context_used: contextRows.length,
    });

    console.log(`ü§ñ Respuesta IA: ${assistant_message}`);
  } catch (error) {
    console.error("‚ùå Error en /chat:", error.message);
    res.status(500).json({ error: "Error interno en /chat", details: error.message });
  }
});

// =========================================================
// üöÄ Iniciar servidor
// =========================================================
const PORT = process.env.PORT || 8080;
app.listen(PORT, async () => {
  await ensureTables();
  console.log(`üî• Servidor corriendo con memoria vectorial e IA en puerto ${PORT}`);
});
